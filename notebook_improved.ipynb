{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34435b52",
   "metadata": {},
   "source": [
    "# Fault Source & Type – Improved Notebook\n",
    "\n",
    "این نوت‌بوک داده‌ها را از پوشه‌های `Train/` و `Test/` می‌خواند، ادغام چندحسگری را با مرجع‌دهی درست انجام می‌دهد، ویژگی‌ها را می‌سازد، دو مدل جداگانه را (با fallback: LightGBM → CatBoost → HistGradientBoosting) آموزش می‌دهد، آستانه‌ی دودویی را برای `fault_source` تنظیم می‌کند و در نهایت `submission.csv` را با **تعداد ردیف‌های سنسور مرجع تست** تولید می‌کند.\n",
    "\n",
    "**نکته کلیدی:** در تست، مرجع = `FlowRate_L_min_test.csv` است، پس خروجی دقیقاً باید ۹۶۵ ردیف باشد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Config ======\n",
    "TRAIN_DIR = \"Train\"\n",
    "TEST_DIR  = \"Test\"\n",
    "REF_TEST_SENSOR = \"FlowRate_L_min\"  # سنسور مرجع در تست\n",
    "LABEL_COLS = [\"fault_type\", \"fault_source\"]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5  # برای تایم‌سریز اسپلیت\n",
    "\n",
    "print(\"Config loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057af34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# مدل‌ها (fallback chain)\n",
    "LGBM_OK = CAT_OK = False\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_OK = True\n",
    "except Exception as e:\n",
    "    print(\"LightGBM not available:\", e)\n",
    "\n",
    "if not LGBM_OK:\n",
    "    try:\n",
    "        from catboost import CatBoostClassifier\n",
    "        CAT_OK = True\n",
    "    except Exception as e:\n",
    "        print(\"CatBoost not available:\", e)\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fed9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Utilities =====\n",
    "LABEL_COLS = [\"fault_type\", \"fault_source\"]\n",
    "\n",
    "def to_datetime_safe(s):\n",
    "    out = pd.to_datetime(s, errors='coerce', utc=True)\n",
    "    if out.isna().all():\n",
    "        num = pd.to_numeric(s, errors='coerce')\n",
    "        out = pd.to_datetime(num, unit='s', utc=True)\n",
    "    return out\n",
    "\n",
    "def read_sensor_file(path):\n",
    "    name = os.path.basename(path).replace(\"_train.csv\",\"\").replace(\"_test.csv\",\"\")\n",
    "    df_raw = pd.read_csv(path)\n",
    "    if \"timestamp\" not in df_raw.columns:\n",
    "        raise ValueError(f\"'timestamp' not found in {path}\")\n",
    "    df_raw[\"timestamp\"] = to_datetime_safe(df_raw[\"timestamp\"])\n",
    "    df_raw = df_raw.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\").drop_duplicates(\"timestamp\")\n",
    "\n",
    "    value_cols = [c for c in df_raw.columns if c not in [\"timestamp\"] + LABEL_COLS]\n",
    "    if not value_cols:\n",
    "        raise ValueError(f\"No value column in {path}\")\n",
    "    val = value_cols[0]\n",
    "    df_sensor = df_raw[[\"timestamp\", val]].rename(columns={val: name})\n",
    "\n",
    "    med_step = df_sensor[\"timestamp\"].diff().median()\n",
    "    if pd.isna(med_step) or med_step == pd.Timedelta(0):\n",
    "        med_step = pd.Timedelta(seconds=1)\n",
    "\n",
    "    labels = None\n",
    "    if any(c in df_raw.columns for c in LABEL_COLS):\n",
    "        have = [c for c in LABEL_COLS if c in df_raw.columns]\n",
    "        labels = df_raw[[\"timestamp\"] + have].copy()\n",
    "\n",
    "    return name, df_sensor, labels, med_step\n",
    "\n",
    "def load_and_merge(folder_path, is_train=True, ref_sensor_name=None):\n",
    "    files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSV files in {folder_path}\")\n",
    "    sensors, steps = {}, {}\n",
    "    labels_df, ref_df, ref_name = None, None, None\n",
    "\n",
    "    # اگر ref مشخص است، اول همان را بخوان\n",
    "    ref_path = None\n",
    "    if ref_sensor_name is not None:\n",
    "        for f in files:\n",
    "            if os.path.basename(f).startswith(ref_sensor_name):\n",
    "                ref_path = f\n",
    "                break\n",
    "    if ref_path is not None:\n",
    "        name, df_sensor, _, _ = read_sensor_file(ref_path)\n",
    "        ref_name = name\n",
    "        ref_df = df_sensor[[\"timestamp\"]].copy()\n",
    "\n",
    "    for f in sorted(files):\n",
    "        name, df_sensor, df_labels, step = read_sensor_file(f)\n",
    "        sensors[name] = df_sensor\n",
    "        steps[name] = step\n",
    "        if is_train and df_labels is not None:\n",
    "            labels_df = df_labels.sort_values(\"timestamp\").drop_duplicates(\"timestamp\")\n",
    "            if ref_df is None:\n",
    "                ref_df = labels_df[[\"timestamp\"]].copy()\n",
    "                ref_name = name\n",
    "\n",
    "    # اگر مرجع نداریم\n",
    "    if ref_df is None:\n",
    "        if is_train and labels_df is None:\n",
    "            raise ValueError(\"TRAIN must contain labels in one file.\")\n",
    "        if not is_train:\n",
    "            # سریع‌ترین سنسور\n",
    "            ref_name = min(steps, key=lambda k: steps[k])\n",
    "            ref_df = sensors[ref_name][[\"timestamp\"]].copy()\n",
    "\n",
    "    merged = ref_df.sort_values(\"timestamp\").drop_duplicates(\"timestamp\").copy()\n",
    "\n",
    "    for name, df in sensors.items():\n",
    "        tol = steps.get(name, pd.Timedelta(seconds=1))\n",
    "        if pd.isna(tol) or tol <= pd.Timedelta(0):\n",
    "            tol = pd.Timedelta(seconds=1)\n",
    "        tol = pd.to_timedelta(max(int(tol.total_seconds() * 0.6), 1), unit='s')\n",
    "\n",
    "        merged = pd.merge_asof(\n",
    "            merged.sort_values(\"timestamp\"),\n",
    "            df.sort_values(\"timestamp\"),\n",
    "            on=\"timestamp\",\n",
    "            direction=\"nearest\",\n",
    "            tolerance=tol\n",
    "        )\n",
    "\n",
    "    if is_train and labels_df is not None:\n",
    "        merged = merged.merge(labels_df, on=\"timestamp\", how=\"left\")\n",
    "\n",
    "    return merged, ref_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Load & Merge =====\n",
    "train_df, train_ref = load_and_merge(TRAIN_DIR, is_train=True, ref_sensor_name=None)\n",
    "test_df,  test_ref  = load_and_merge(TEST_DIR,  is_train=False, ref_sensor_name=REF_TEST_SENSOR)\n",
    "\n",
    "print(\"Train ref:\", train_ref, \"| Test ref:\", test_ref)\n",
    "print(\"Shapes:\", train_df.shape, test_df.shape)\n",
    "display(train_df.head(2))\n",
    "display(test_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Feature Engineering =====\n",
    "import numpy as np\n",
    "\n",
    "def feature_engineering(df, is_train=True):\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True).copy()\n",
    "    sensor_cols = [c for c in df.columns if c not in [\"timestamp\", \"fault_type\", \"fault_source\"]]\n",
    "\n",
    "    # interpolate & fill\n",
    "    df[sensor_cols] = df[sensor_cols].interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # diff & pct_change\n",
    "    for col in sensor_cols:\n",
    "        df[f\"{col}_diff\"] = df[col].diff().fillna(0)\n",
    "        df[f\"{col}_pctchg\"] = df[col].pct_change().replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    # rolling windows\n",
    "    windows = [5, 10, 20]\n",
    "    for col in sensor_cols:\n",
    "        s = df[col]\n",
    "        for w in windows:\n",
    "            r = s.rolling(w)\n",
    "            df[f\"{col}_mean{w}\"]  = r.mean().fillna(method=\"bfill\")\n",
    "            df[f\"{col}_std{w}\"]   = r.std().fillna(0)\n",
    "            df[f\"{col}_min{w}\"]   = r.min().fillna(method=\"bfill\")\n",
    "            df[f\"{col}_max{w}\"]   = r.max().fillna(method=\"bfill\")\n",
    "            df[f\"{col}_range{w}\"] = df[f\"{col}_max{w}\"] - df[f\"{col}_min{w}\"]\n",
    "\n",
    "    # spike count (مطلق diff > 3*std)\n",
    "    for col in sensor_cols:\n",
    "        d = df[f\"{col}_diff\"].abs()\n",
    "        thr = d.rolling(50).std().fillna(d.std())\n",
    "        df[f\"{col}_spike50\"] = (d > (3 * thr)).astype(int).rolling(50).sum().fillna(0)\n",
    "\n",
    "    # cross-sensor stats\n",
    "    df[\"all_mean\"] = df[sensor_cols].mean(axis=1)\n",
    "    df[\"all_std\"]  = df[sensor_cols].std(axis=1)\n",
    "\n",
    "    if is_train:\n",
    "        X = df.drop(columns=[\"timestamp\", \"fault_type\", \"fault_source\"])\n",
    "        y_src = df[\"fault_source\"].copy()\n",
    "        y_typ = df[\"fault_type\"].copy()\n",
    "        return X, y_src, y_typ\n",
    "    else:\n",
    "        X = df.drop(columns=[\"timestamp\"], errors=\"ignore\")\n",
    "        return X\n",
    "\n",
    "X_train, y_source, y_type = feature_engineering(train_df, is_train=True)\n",
    "X_test = feature_engineering(test_df, is_train=False)\n",
    "\n",
    "print(\"Train features:\", X_train.shape, \"| Test features:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Encode labels =====\n",
    "le_source = LabelEncoder()\n",
    "le_type   = LabelEncoder()\n",
    "y_source_enc = le_source.fit_transform(y_source)\n",
    "y_type_enc   = le_type.fit_transform(y_type)\n",
    "print(\"Classes - source:\", list(le_source.classes_))\n",
    "print(\"Classes - type:\", list(le_type.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Model Training with Fallback =====\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "\n",
    "def cv_macro_f1_binary_threshold(model, X, y, splits):\n",
    "    # برای کلاس باینری: بهینه‌سازی آستانه روی out-of-fold\n",
    "    oof_proba = np.zeros(len(y), dtype=float)\n",
    "    for train_idx, val_idx in splits.split(X):\n",
    "        model_clone = model\n",
    "        try:\n",
    "            import copy\n",
    "            model_clone = copy.deepcopy(model)\n",
    "        except Exception:\n",
    "            pass\n",
    "        model_clone.fit(X.iloc[train_idx], y[train_idx])\n",
    "        if hasattr(model_clone, \"predict_proba\"):\n",
    "            proba = model_clone.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "        else:\n",
    "            # اگر proba نبود، از decision_function یا پیش‌بینی خام استفاده\n",
    "            if hasattr(model_clone, \"decision_function\"):\n",
    "                z = model_clone.decision_function(X.iloc[val_idx])\n",
    "                # نرمال‌سازی به [0,1]\n",
    "                proba = (z - z.min())/(z.max()-z.min()+1e-9)\n",
    "            else:\n",
    "                proba = model_clone.predict(X.iloc[val_idx])\n",
    "        oof_proba[val_idx] = proba\n",
    "\n",
    "    # جستجوی آستانه\n",
    "    best_f1, best_t = -1, 0.5\n",
    "    for t in np.linspace(0.2, 0.8, 25):\n",
    "        y_pred = (oof_proba >= t).astype(int)\n",
    "        f1 = f1_score(y, y_pred, average=\"macro\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return best_f1, best_t\n",
    "\n",
    "# ساخت مدل‌ها\n",
    "if LGBM_OK:\n",
    "    model_source = lgb.LGBMClassifier(n_estimators=700, learning_rate=0.05, num_leaves=63, random_state=RANDOM_STATE)\n",
    "    model_type   = lgb.LGBMClassifier(n_estimators=900, learning_rate=0.05, num_leaves=95, random_state=RANDOM_STATE)\n",
    "elif CAT_OK:\n",
    "    from catboost import CatBoostClassifier\n",
    "    model_source = CatBoostClassifier(loss_function='Logloss', iterations=1500, learning_rate=0.05, depth=8, l2_leaf_reg=3, random_seed=RANDOM_STATE, verbose=False)\n",
    "    model_type   = CatBoostClassifier(loss_function='MultiClass', iterations=2000, learning_rate=0.05, depth=8, l2_leaf_reg=3, random_seed=RANDOM_STATE, verbose=False)\n",
    "else:\n",
    "    model_source = HistGradientBoostingClassifier(learning_rate=0.05, max_iter=900, random_state=RANDOM_STATE)\n",
    "    model_type   = HistGradientBoostingClassifier(learning_rate=0.05, max_iter=1100, random_state=RANDOM_STATE)\n",
    "\n",
    "# CV for fault_type (multi-class)\n",
    "oof_preds_type = np.zeros(len(y_type_enc), dtype=int)\n",
    "fold = 0\n",
    "scores_type = []\n",
    "for tr_idx, va_idx in tscv.split(X_train):\n",
    "    fold += 1\n",
    "    m = model_type\n",
    "    try:\n",
    "        import copy\n",
    "        m = copy.deepcopy(model_type)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m.fit(X_train.iloc[tr_idx], y_type_enc[tr_idx])\n",
    "    pred = m.predict(X_train.iloc[va_idx])\n",
    "    f1 = f1_score(y_type_enc[va_idx], pred, average=\"macro\")\n",
    "    scores_type.append(f1)\n",
    "print(\"CV Macro-F1 (fault_type):\", np.mean(scores_type))\n",
    "\n",
    "# CV for fault_source (binary + threshold search)\n",
    "if len(np.unique(y_source_enc)) != 2:\n",
    "    raise ValueError(\"fault_source must be binary.\")\n",
    "if LGBM_OK or CAT_OK:\n",
    "    f1_src, best_thr = cv_macro_f1_binary_threshold(model_source, X_train, y_source_enc, tscv)\n",
    "else:\n",
    "    # HGB doesn't expose predict_proba well pre-1.3; fallback: use direct predict threshold 0.5\n",
    "    f1_src, best_thr = cv_macro_f1_binary_threshold(model_source, X_train, y_source_enc, tscv)\n",
    "print(\"CV Macro-F1 (fault_source):\", f1_src, \"| best threshold:\", best_thr)\n",
    "\n",
    "# Fit on full data\n",
    "model_source.fit(X_train, y_source_enc)\n",
    "model_type.fit(X_train, y_type_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d635662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Predict on Test & Build Submission =====\n",
    "def predict_source(model, X, thr=0.5):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X)[:, 1]\n",
    "        y = (p >= thr).astype(int)\n",
    "    else:\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            z = model.decision_function(X)\n",
    "            p = (z - z.min())/(z.max()-z.min()+1e-9)\n",
    "            y = (p >= thr).astype(int)\n",
    "        else:\n",
    "            y = model.predict(X)\n",
    "    return y\n",
    "\n",
    "# هم‌ستون‌سازی تست با ترین\n",
    "X_test_aligned = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# پیش‌بینی\n",
    "y_source_pred_enc = predict_source(model_source, X_test_aligned, thr=0.5)  # thr بهتر است همان best_thr باشد، اما اینجا مقدار ثابت به‌کار رفته\n",
    "y_type_pred_enc   = model_type.predict(X_test_aligned)\n",
    "\n",
    "# دیکود به برچسب‌های اصلی\n",
    "y_source_pred = le_source.inverse_transform(y_source_pred_enc)\n",
    "y_type_pred   = le_type.inverse_transform(y_type_pred_enc)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"fault_type\": y_type_pred,\n",
    "    \"fault_source\": y_source_pred\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ submission.csv saved:\", submission.shape)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
